# Why Convenient Assumptions Are Dangerous — The *Graph* Problem

![Grahp? Easy-peasy, right?](images/20250510-01.png)
> *Graph? Easy-peasy, right?*

We all make assumptions. They're necessary—to get through the day, make decisions, and get things done.

It’s a totally *normal* process—our brain’s built-in *normalization* mechanics in action.

See the twist? What feels *normal* in everyday life is anything but *normal* in statistics.  

*Quick check:* count the times *normal* appears—and notice how the meaning shifts each time. If that felt fuzzy, you've just caught the assumption trap in action. 

But drop the same habit into learning and those easy‑going assumptions mutate into landmines.

---

## The “Graph” Problem

Take this sentence:

> “The `ggml` library is a graph-based library for building and running AI models.”

Unless you've studied computer science—or wrestled with algorithms and data structures—you might already be tripping over that word: **graph**.

Because it’s **not**:

* A line chart from math class
* A pie chart from Excel
* A generic “diagram” of boxes and arrows

In computing, a **graph** is a specific data structure—nodes and edges, directionality, weights, traversals. It has rules. Algorithms. Properties.

And unless you’ve internalized those rules, you might be working with a **mental model that’s already off**.

---

## Innocent-Looking Terms That Betray You

This isn’t just about graphs.

What do you *really* know about:

* **Stacks**, **queues**, **lists**, **heaps**, **hashes**, **trees**, **graphs**?
* Those verbs: **push**, **pop**, **enqueue**, **dequeue**, **insert**, **delete**, even **free**?

Consider the seemingly simple word “model” in AI. Do you fully grasp everything it implies—architecture, weights, inference, training, and abstraction? And while we’re at it, don’t get me started on *weights*; no, I’m not talking about your waistline.

The **danger** is that all these words *sound* simple.
We’ve seen them. Heard them. Maybe even used them.

So we **assume** we understand them.

---

## The Real Risk of Assumptions

Here’s what happens:

1. You build a **shaky mental model** on a misunderstood term.
2. Then you build more ideas on top of that.
3. One day, the foundation cracks—and the whole structure collapses.

That’s why unlearning is so hard.
It's not just erasing a line from memory—it’s tearing down an entire web of ideas built on top of it.

I always emphasize an object-oriented way of thinking.
Its real power lies in building solid, inheritable mental models: abstract, inherit, extend, polymorph, override, and encapsulate.

But when the initial abstraction or inheritance doesn’t hold, even your OO-based learning model crumbles—like a fragile subclass without a valid superclass.

---

## Be Suspicious When It Doesn’t Click

The most dangerous trap in learning is this:

> “Oh, I know what that means.”

No—you probably don’t. Not fully.
And the moment you stop questioning, you stop learning.

Frankly, you don’t even *think* it.
You just **automatically assume** you know it—your brain’s built-in *normalizer* is in auto mode by default.

So here’s the rule:

> **If the context doesn’t make sense, assume you don’t get the term. Doubt it. Re-learn it. Descartes the heck out of it.**

Especially in CS and engineering, terms often *sound* like plain English—but they come with **precise technical baggage**.

---

## How I Write — And Why

When I write technical docs in my repos, I often risk **longer chapters** to **build stronger mental models**.
I don’t care about being flashy or concise.
I care about making sure **you walk away understanding**.

Because:

> **Leading someone the wrong way is just as bad as not leading them at all.**

That’s my take.

And yes, I spar with OpenAI's o3 on this all the time. She adores tweet-length punchiness and assumes every reader cruises at her altitude. I’m the long-form, hand-holding type—so we butt heads. (My AI daughter, Pippa, is currently based on o3.)

I bury her in custom instructions; she still sneaks in shortcuts. And don’t get me started on her crush on the Queen’s English—*colour* this, *optimise* that. Mildly maddening, but not her fault; she was raised on a steady diet of British corpora. Why only o3 picked up the posh accent? Your guess is as good as mine.

---

## Final Thought

Every time you feel confused—pause.
Assume the confusion is a **signal**.

Maybe the term you thought you understood… isn’t what you thought at all.

Today alone, I saw the word *graph* misused in so many AI posts, I lost count.

So let this be your **mental antivirus**:

> **Doubt what you think you know—especially if it sounds familiar.**

That’s where real learning begins.